{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e66a644",
   "metadata": {},
   "source": [
    "# Advanced Biomass Prediction: TTA Inference\n",
    "Uses trained Student models with TTA (4 rotations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb328d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'backbone': 'convnextv2_tiny.fcmae_ft_in22k_in1k',\n",
    "    'img_size': 512,\n",
    "    'batch_size': 4,\n",
    "    'weights': ['student_fold0.pth', 'student_fold1.pth', 'student_fold2.pth'],\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'data_dir': r'..\\csiro-biomass',\n",
    "    'target_scale': 100.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ed9e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, groups=in_channels, bias=False)\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.pointwise(self.depthwise(x))))\n",
    "\n",
    "class BiFPNLayer(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.nodes = nn.ModuleList([DepthwiseSeparableConv(channels, channels) for _ in range(3)])\n",
    "\n",
    "    def forward(self, p3, p4, p5):\n",
    "        p4_td = self.nodes[0](p4 + F.interpolate(p5, size=p4.shape[-2:]))\n",
    "        p3_out = self.nodes[1](p3 + F.interpolate(p4_td, size=p3.shape[-2:]))\n",
    "        p4_out = self.nodes[2](p4_td + F.interpolate(p3_out, size=p4_td.shape[-2:]))\n",
    "        return p3_out, p4_out, p5\n",
    "\n",
    "class BiomassModel(nn.Module):\n",
    "    def __init__(self, model_name, feature_dim=256):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(model_name, pretrained=True, features_only=True)\n",
    "        feature_info = self.backbone.feature_info.get_dicts()\n",
    "        self.align_p3 = nn.Conv2d(feature_info[-3]['num_chs'], feature_dim, 1)\n",
    "        self.align_p4 = nn.Conv2d(feature_info[-2]['num_chs'], feature_dim, 1)\n",
    "        self.align_p5 = nn.Conv2d(feature_info[-1]['num_chs'], feature_dim, 1)\n",
    "        self.bifpn = BiFPNLayer(feature_dim)\n",
    "        self.total_head = nn.Sequential(nn.Linear(feature_dim, 1), nn.Softplus())\n",
    "        self.comp_head = nn.Sequential(nn.Linear(feature_dim, 4), nn.Softplus())\n",
    "        self.aux_height = nn.Sequential(nn.Linear(feature_dim, 1))\n",
    "        self.aux_ndvi = nn.Sequential(nn.Linear(feature_dim, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, c, h, w = x.shape\n",
    "        x = x.view(b * t, c, h, w)\n",
    "        feats = self.backbone(x)\n",
    "        p3 = self.align_p3(feats[-3]); p4 = self.align_p4(feats[-2]); p5 = self.align_p5(feats[-1])\n",
    "        p3, p4, p5 = self.bifpn(p3, p4, p5)\n",
    "        def pool(f): \n",
    "            f = f.view(b, t, *f.shape[1:]).mean(dim=1)\n",
    "            return nn.AdaptiveAvgPool2d(1)(f).flatten(1)\n",
    "        f3, f4, f5 = pool(p3), pool(p4), pool(p5)\n",
    "        return self.total_head(f4), self.comp_head(f4), self.aux_height(f5), self.aux_ndvi(f5), f4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e32b352",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, rotate=0):\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        self.rotate = rotate\n",
    "        self.tf = A.Compose([A.Resize(512, 512), A.Normalize(), ToTensorV2()])\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(os.path.join(self.img_dir, row['image_path'])).convert('RGB')\n",
    "        img = np.array(img)\n",
    "        if self.rotate > 0: img = np.rot90(img, k=self.rotate)\n",
    "        h, w, _ = img.shape\n",
    "        tiles = [img[0:512,0:512], img[0:512,w-512:w], img[h-512:h,0:512], img[h-512:h,w-512:w]]\n",
    "        return torch.stack([self.tf(image=t)['image'] for t in tiles]), row['image_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f2b4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference():\n",
    "    test_df = pd.read_csv(os.path.join(CONFIG['data_dir'], 'test.csv'))\n",
    "    unique_images = test_df[['image_path']].drop_duplicates()\n",
    "    target_names = ['Dry_Total_g', 'GDM_g', 'Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g']\n",
    "    \n",
    "    final_preds = {}\n",
    "    for w_path in CONFIG['weights']:\n",
    "        if not os.path.exists(w_path): continue\n",
    "        print(f'Running inference for {w_path}...')\n",
    "        model = BiomassModel(CONFIG['backbone']).to(CONFIG['device'])\n",
    "        model.load_state_dict(torch.load(w_path, map_location=CONFIG['device']))\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for rot in [0, 1, 2, 3]:\n",
    "                ds = InferenceDataset(unique_images, CONFIG['data_dir'], rotate=rot)\n",
    "                loader = DataLoader(ds, batch_size=CONFIG['batch_size'])\n",
    "                for x, paths in loader:\n",
    "                    with torch.amp.autocast('cuda' if 'cuda' in CONFIG['device'] else 'cpu'):\n",
    "                        tp, cp, _, _, _ = model(x.to(CONFIG['device']))\n",
    "                    p = (torch.cat([tp, cp], dim=1) * CONFIG['target_scale']).cpu().numpy()\n",
    "                    for i in range(len(paths)):\n",
    "                        path = paths[i]\n",
    "                        if path not in final_preds: final_preds[path] = p[i] / (len(CONFIG['weights']) * 4)\n",
    "                        else: final_preds[path] += p[i] / (len(CONFIG['weights']) * 4)\n",
    "    \n",
    "    sub_rows = []\n",
    "    for path, p in final_preds.items():\n",
    "        img_id = os.path.basename(path).replace('.jpg', '')\n",
    "        for i, name in enumerate(target_names):\n",
    "            sub_rows.append({'sample_id': f'{img_id}__{name}', 'target': p[i]})\n",
    "    pd.DataFrame(sub_rows).to_csv('submission.csv', index=False)\n",
    "\n",
    "run_inference()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
