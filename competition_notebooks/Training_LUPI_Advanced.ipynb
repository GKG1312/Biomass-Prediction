{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4ada984",
   "metadata": {},
   "source": [
    "# Advanced Biomass Prediction: LUPI & Multi-Modal Distillation\n",
    "This notebook implements the training pipeline described in the 'Advanced Computer Vision Frameworks for Pasture Biomass Estimation' report.\n",
    "\n",
    "### Key Features:\n",
    "- **Backbone**: ConvNeXt V2-Base\n",
    "- **Tiling**: 4x 512x512 crops to preserve texture details\n",
    "- **Neck**: BiFPN for multi-scale feature fusion\n",
    "- **LUPI**: Learning Using Privileged Information (NDVI & Height)\n",
    "- **Knowledge Distillation**: Teacher-Student framework\n",
    "- **Losses**: Weighted Huber + Hierarchical Consistency + KD Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c71d352",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'seed': 42,\n",
    "    'backbone': 'convnextv2_tiny.fcmae_ft_in22k_in1k',\n",
    "    'img_size': 512,\n",
    "    'batch_size': 4,\n",
    "    'data_dir': r'..\\csiro-biomass',\n",
    "    'epochs_teacher': 15,\n",
    "    'epochs_distill': 25,\n",
    "    'epochs_finetune': 10,\n",
    "    'lr': 2e-4,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'n_splits': 3,\n",
    "    'target_cols': ['Dry_Total_g', 'GDM_g', 'Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g'],\n",
    "    'weights': torch.tensor([0.5, 0.2, 0.1, 0.1, 0.1]),\n",
    "    'use_amp': True,\n",
    "    'target_scale': 100.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b50f9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, groups=in_channels, bias=False)\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.pointwise(self.depthwise(x))))\n",
    "\n",
    "class BiFPNLayer(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.nodes = nn.ModuleList([DepthwiseSeparableConv(channels, channels) for _ in range(3)])\n",
    "\n",
    "    def forward(self, p3, p4, p5):\n",
    "        p4_td = self.nodes[0](p4 + F.interpolate(p5, size=p4.shape[-2:]))\n",
    "        p3_out = self.nodes[1](p3 + F.interpolate(p4_td, size=p3.shape[-2:]))\n",
    "        p4_out = self.nodes[2](p4_td + F.interpolate(p3_out, size=p4_td.shape[-2:]))\n",
    "        return p3_out, p4_out, p5\n",
    "\n",
    "class BiomassModel(nn.Module):\n",
    "    def __init__(self, model_name, feature_dim=256):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(model_name, pretrained=True, features_only=True)\n",
    "        feature_info = self.backbone.feature_info.get_dicts()\n",
    "        self.align_p3 = nn.Conv2d(feature_info[-3]['num_chs'], feature_dim, 1)\n",
    "        self.align_p4 = nn.Conv2d(feature_info[-2]['num_chs'], feature_dim, 1)\n",
    "        self.align_p5 = nn.Conv2d(feature_info[-1]['num_chs'], feature_dim, 1)\n",
    "        self.bifpn = BiFPNLayer(feature_dim)\n",
    "        self.total_head = nn.Sequential(nn.Linear(feature_dim, 1), nn.Softplus())\n",
    "        self.comp_head = nn.Sequential(nn.Linear(feature_dim, 4), nn.Softplus())\n",
    "        self.aux_height = nn.Sequential(nn.Linear(feature_dim, 1))\n",
    "        self.aux_ndvi = nn.Sequential(nn.Linear(feature_dim, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, c, h, w = x.shape\n",
    "        x = x.view(b * t, c, h, w)\n",
    "        feats = self.backbone(x)\n",
    "        p3 = self.align_p3(feats[-3]); p4 = self.align_p4(feats[-2]); p5 = self.align_p5(feats[-1])\n",
    "        p3, p4, p5 = self.bifpn(p3, p4, p5)\n",
    "        def pool(f): \n",
    "            f = f.view(b, t, *f.shape[1:]).mean(dim=1)\n",
    "            return nn.AdaptiveAvgPool2d(1)(f).flatten(1)\n",
    "        f3, f4, f5 = pool(p3), pool(p4), pool(p5)\n",
    "        return self.total_head(f4), self.comp_head(f4), self.aux_height(f5), self.aux_ndvi(f5), f4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670d0e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TiledBiomassDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, target_cols, species_map, transform=None):\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        self.target_cols = target_cols\n",
    "        self.species_map = species_map\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.img_dir, row['image_path'])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image = np.array(image)\n",
    "        h, w, _ = image.shape\n",
    "        tiles = [\n",
    "            image[0:512, 0:512], image[0:512, w-512:w], \n",
    "            image[h-512:h, 0:512], image[h-512:h, w-512:w]\n",
    "        ]\n",
    "        if self.transform:\n",
    "            tiles = [self.transform(image=t)['image'] for t in tiles]\n",
    "        else:\n",
    "            tf = A.Compose([A.Resize(512, 512), A.Normalize(), ToTensorV2()])\n",
    "            tiles = [tf(image=t)['image'] for t in tiles]\n",
    "        \n",
    "        tiles = torch.stack(tiles)\n",
    "        targets = torch.tensor(row[self.target_cols].values.astype(np.float32)) / CONFIG['target_scale']\n",
    "        \n",
    "        species_id = self.species_map.get(row['Species'], 0)\n",
    "        meta = torch.tensor([float(row['Pre_GSHH_NDVI']), float(row['Height_Ave_cm']), float(species_id)], dtype=torch.float32)\n",
    "        return tiles, targets, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea69043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(img_size, is_train=True):\n",
    "    if is_train:\n",
    "        return A.Compose([\n",
    "            A.HorizontalFlip(p=0.5), A.VerticalFlip(p=0.5), A.RandomRotate90(p=0.5),\n",
    "            A.RandomShadow(p=0.3), A.ColorJitter(brightness=0.2, contrast=0.2, p=0.4),\n",
    "            A.Normalize(), ToTensorV2()\n",
    "        ])\n",
    "    else:\n",
    "        return A.Compose([A.Normalize(), ToTensorV2()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0653f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TeacherModel(nn.Module):\n",
    "    def __init__(self, student, num_species):\n",
    "        super().__init__()\n",
    "        self.student = student\n",
    "        self.species_embed = nn.Embedding(num_species, 32)\n",
    "        self.meta_embed = nn.Sequential(\n",
    "            nn.Linear(2 + 32, 64), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 256)\n",
    "        )\n",
    "    def forward(self, x, meta):\n",
    "        _, _, _, _, feat = self.student(x) # feat is [B, 256]\n",
    "        s_idx = meta[:, 2].long()\n",
    "        s_emb = self.species_embed(s_idx)\n",
    "        m_in = torch.cat([meta[:, :2], s_emb], dim=1)\n",
    "        m_feat = self.meta_embed(m_in)\n",
    "        f = feat + m_feat\n",
    "        return self.student.total_head(f), self.student.comp_head(f), f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b097d06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalLoss(nn.Module):\n",
    "    def __init__(self, weights):\n",
    "        super().__init__()\n",
    "        self.weights = weights.to(CONFIG['device'])\n",
    "        self.huber = nn.HuberLoss()\n",
    "    def forward(self, total_p, comps_p, targets):\n",
    "        # targets order: [Total, GDM, Green, Dead, Clover]\n",
    "        l_total = self.huber(total_p.squeeze(), targets[:, 0])\n",
    "        l_comps = self.huber(comps_p, targets[:, 1:])\n",
    "        # 0: GDM, 1: Green, 2: Dead, 3: Clover in comps_p\n",
    "        l_cons1 = F.mse_loss(total_p.squeeze(), comps_p[:, 1] + comps_p[:, 2]) # Total = Green + Dead\n",
    "        l_cons2 = F.mse_loss(comps_p[:, 1], comps_p[:, 0] + comps_p[:, 3])    # Green = GDM + Clover\n",
    "        return 0.5 * l_total + 0.5 * l_comps.mean() + 0.1 * (l_cons1 + l_cons2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd45859",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(CONFIG['data_dir'], 'train.csv'))\n",
    "df_wide = train_df.pivot_table(index=['image_path', 'Sampling_Date', 'State', 'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm'], \n",
    "                              columns='target_name', values='target').reset_index()\n",
    "df_wide['group'] = df_wide['Sampling_Date'] + '_' + df_wide['State']\n",
    "species_map = {s: i for i, s in enumerate(sorted(df_wide['Species'].unique()))}\n",
    "num_species = len(species_map)\n",
    "sgkf = StratifiedGroupKFold(n_splits=CONFIG['n_splits'])\n",
    "df_wide['fold'] = -1\n",
    "y_bins = pd.cut(df_wide['Dry_Total_g'], bins=10, labels=False)\n",
    "for f, (t_, v_) in enumerate(sgkf.split(df_wide, y_bins, groups=df_wide['group'])):\n",
    "    df_wide.loc[v_, 'fold'] = f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2a7d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_fold(fold):\n",
    "    train_ds = TiledBiomassDataset(df_wide[df_wide.fold != fold], CONFIG['data_dir'], CONFIG['target_cols'], species_map, get_transforms(512, True))\n",
    "    val_ds = TiledBiomassDataset(df_wide[df_wide.fold == fold], CONFIG['data_dir'], CONFIG['target_cols'], species_map, get_transforms(512, False))\n",
    "    loader = DataLoader(train_ds, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=2, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=2)\n",
    "    \n",
    "    student = BiomassModel(CONFIG['backbone']).to(CONFIG['device'])\n",
    "    teacher = TeacherModel(student, num_species).to(CONFIG['device'])\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        teacher = nn.DataParallel(teacher)\n",
    "        student_dp = nn.DataParallel(student)\n",
    "    else:\n",
    "        student_dp = student\n",
    "\n",
    "    opt = torch.optim.AdamW(teacher.parameters(), lr=CONFIG['lr'])\n",
    "    criterion = HierarchicalLoss(CONFIG['weights'])\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=CONFIG['use_amp'])\n",
    "\n",
    "    def validate(m, loader_v):\n",
    "        m.eval(); preds, truths = [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y, met in loader_v:\n",
    "                with torch.amp.autocast('cuda', enabled=CONFIG['use_amp']):\n",
    "                    out, _, _, _, _ = student_dp(x.to(CONFIG['device']))\n",
    "                preds.append(out.cpu().numpy()); truths.append(y[:, 0].numpy())\n",
    "        preds = np.concatenate(preds).flatten()\n",
    "        truths = np.concatenate(truths).flatten()\n",
    "        if len(np.unique(preds)) <= 1: return 0.0\n",
    "        return np.corrcoef(preds, truths)[0,1]**2\n",
    "\n",
    "    print(f'Training Teacher Fold {fold}...')\n",
    "    for epoch in range(CONFIG['epochs_teacher']):\n",
    "        teacher.train(); epoch_loss = 0\n",
    "        for x, y, m in loader:\n",
    "            x, y, m = x.to(CONFIG['device']), y.to(CONFIG['device']), m.to(CONFIG['device'])\n",
    "            opt.zero_grad()\n",
    "            with torch.amp.autocast('cuda', enabled=CONFIG['use_amp']):\n",
    "                tp, cp, _ = teacher(x, m)\n",
    "                loss = criterion(tp, cp, y)\n",
    "            scaler.scale(loss).backward(); scaler.step(opt); scaler.update()\n",
    "            epoch_loss += loss.item()\n",
    "        val_r2 = validate(teacher, val_loader)\n",
    "        print(f'Teacher Ep {epoch+1} | Loss: {epoch_loss/len(loader):.4f} | Val R2: {val_r2:.4f}')\n",
    "\n",
    "    print(f'Distilling Student Fold {fold}...')\n",
    "    opt_s = torch.optim.AdamW(student.parameters(), lr=CONFIG['lr'])\n",
    "    for epoch in range(CONFIG['epochs_distill']):\n",
    "        student.train(); epoch_loss = 0\n",
    "        for x, y, m in loader:\n",
    "            x, y, m = x.to(CONFIG['device']), y.to(CONFIG['device']), m.to(CONFIG['device'])\n",
    "            opt_s.zero_grad()\n",
    "            with torch.amp.autocast('cuda', enabled=CONFIG['use_amp']):\n",
    "                with torch.no_grad(): tp_t, cp_t, f_t = teacher(x, m)\n",
    "                tp_s, cp_s, h_s, n_s, f_s = student_dp(x)\n",
    "                loss = criterion(tp_s, cp_s, y) + 0.5*F.mse_loss(f_s, f_t) + 0.1*F.mse_loss(h_s.squeeze(), m[:, 1]/CONFIG['target_scale'])\n",
    "            scaler.scale(loss).backward(); scaler.step(opt_s); scaler.update()\n",
    "            epoch_loss += loss.item()\n",
    "        val_r2 = validate(student_dp, val_loader)\n",
    "        print(f'Student Ep {epoch+1} | Loss: {epoch_loss/len(loader):.4f} | Val R2: {val_r2:.4f}')\n",
    "    \n",
    "    torch.save(student.state_dict(), f'student_fold{fold}.pth')\n",
    "\n",
    "for f in range(CONFIG['n_splits']):\n",
    "    train_one_fold(f)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
