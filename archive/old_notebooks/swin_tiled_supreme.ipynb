{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 112509,
          "databundleVersionId": 14254895,
          "sourceType": "competition"
        },
        {
          "sourceId": 715929,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 544188,
          "modelId": 557286
        }
      ],
      "dockerImageVersionId": 31236,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# CSIRO Swin-V2 Wide-Tiled Supreme\n\n**Architecture Update:**\n1. **Fixed Aspect Ratio**: Both Global and Tiled inputs are now 384x768.\n2. **Wide Tiles**: Concatenates Left and Right high-res crops horizontally for Swin-V2 compatibility.\n3. **Swin-V2-Base**: Stable multi-stream fusion logic.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import os, sys, functools, json\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport timm\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom sklearn.model_selection import GroupKFold\n\nprint = functools.partial(print, flush=True)\nDATA_DIR = '/kaggle/input/csiro-biomass'\nCHECKPOINT_DIR = './models_checkpoints'\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)\n\nTARGET_COLUMNS = ['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'GDM_g', 'Dry_Total_g']\n\nCONFIG = {\n    'model_name': 'swinv2_base_window12_192.ms_in22k', \n    'img_h': 384, 'img_w': 768,\n    'batch_size': 8, 'lr': 1e-5, 'epochs': 30, 'mixup_prob': 0.5, \n    'device': 'cuda', 'n_splits': 5,\n    'resume_path': None # Set to None to avoid data leakage and get true R2\n}",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-11T06:49:08.843858Z",
          "iopub.execute_input": "2026-01-11T06:49:08.844071Z",
          "iopub.status.idle": "2026-01-11T06:49:23.867110Z",
          "shell.execute_reply.started": "2026-01-11T06:49:08.844049Z",
          "shell.execute_reply": "2026-01-11T06:49:23.866267Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "class WideTiledSwin(nn.Module):\n    def __init__(self, model_name=CONFIG['model_name'], num_species=15):\n        super().__init__()\n        # The model is initialized for 384x768 resolution\n        self.backbone = timm.create_model(model_name, pretrained=True, num_classes=0, img_size=(CONFIG['img_h'], CONFIG['img_w']))\n        d = self.backbone.num_features\n        \n        self.meta_reg = nn.Linear(d*2, 2); self.meta_cls = nn.Linear(d*2, num_species)\n        self.species_emb = nn.Embedding(num_species, 32)\n        \n        fusion_dim = d*2 + 2 + 32\n        self.heads = nn.ModuleList([nn.Sequential(nn.Linear(fusion_dim, 512), nn.GELU(), nn.Linear(512, 1)) for _ in range(5)])\n        \n    def forward(self, x_g, x_wide_tule):\n        # x_g (B, 3, 384, 768) - Global resized view\n        # x_wide_tule (B, 3, 384, 768) - Two high-res tiles concatenated horizontally\n        fg = self.backbone(x_g)\n        ft = self.backbone(x_wide_tule)\n        \n        vis = torch.cat([fg, ft], dim=1)\n        pr, pc = self.meta_reg(vis), self.meta_cls(vis)\n        se = self.species_emb(torch.argmax(pc, dim=1))\n        f = torch.cat([vis, pr, se], dim=1)\n        return torch.cat([h(f) for h in self.heads], dim=1), pr, pc",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "class WideTiledDs(Dataset):\n    def __init__(self, df, tf, tile_tf, sm):\n        self.df, self.tf, self.tile_tf, self.sm = df, tf, tile_tf, sm\n        self.df, self.tf, self.sm = df, tf, sm\n    def __len__(self): return len(self.df)\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_pill = Image.open(os.path.join(DATA_DIR, row['image_path'])).convert('RGB')\n        img_np = np.array(img_pill)\n        W = img_np.shape[1]; mid = W // 2\n        \n        # Global view: Resize original image to target resolution\n        g = self.tf(image=img_np)['image']\n        \n        # Wide-Tile view: Capture high-res from original and Concat horizontally\n        # This creates a wide aspect ratio image from high-res patches\n        # Real Tiling: Capture high-res crops and resize them individually before concatenation\n        left_tile = img_np[:, :mid, :]\n        right_tile = img_np[:, mid:, :]\n        # Resize individually to capture fine details\n        lt_res = self.tile_tf(image=left_tile)['image'] # This tf should resize to 384x384 or similar\n        rt_res = self.tile_tf(image=right_tile)['image']\n        t = torch.cat([lt_res, rt_res], dim=2) # Concat horizontally in tensor space result is (3, 384, 768)\n        \n        b = torch.tensor(row[TARGET_COLUMNS].values.astype(np.float32))\n        r = torch.tensor([row['Pre_GSHH_NDVI'], row['Height_Ave_cm']], dtype=torch.float32)\n        c = torch.tensor(self.sm[row['Species']], dtype=torch.long)\n        return g, t, b, r, c",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "def competition_metric(y_true, y_pred):\n    weights = np.array([0.1, 0.1, 0.1, 0.2, 0.5])\n    y_true_flat = y_true.flatten()\n    y_pred_flat = y_pred.flatten()\n    w_flat = np.tile(weights, (len(y_true), 1)).flatten()\n    \n    y_avg_w = np.sum(w_flat * y_true_flat) / np.sum(w_flat)\n    ss_res = np.sum(w_flat * (y_true_flat - y_pred_flat)**2)\n    ss_tot = np.sum(w_flat * (y_true_flat - y_avg_w)**2)\n    return 1 - ss_res / ss_tot if ss_tot != 0 else 0.0\n\ndef apply_mixup(g, t, b, r, c, ns):\n    if np.random.rand() > CONFIG['mixup_prob']: \n        return g, t, b, r, nn.functional.one_hot(c, ns).float(), 1.0\n    i = torch.randperm(g.size(0)).to(g.device); l = np.random.beta(1.0, 1.0)\n    g, t = l*g + (1-l)*g[i], l*t + (1-l)*t[i]\n    b, r = l*b + (1-l)*b[i], l*r + (1-l)*r[i]\n    c_oh = nn.functional.one_hot(c, ns).float()\n    return g, t, b, r, l*c_oh + (1-l)*c_oh[i], l\n\ndef train_fold(fold, t_df, v_df, sm):\n    ns = len(sm)\n    # Shared transformation for both views since they have identical target resolutions now\n    tf = A.Compose([A.Resize(CONFIG['img_h'], CONFIG['img_w']), A.HorizontalFlip(), A.Normalize(), ToTensorV2()])\n    # Special transform for 1:1 tiles\n    tile_tf = A.Compose([A.Resize(CONFIG['img_h'], CONFIG['img_h']), A.HorizontalFlip(), A.Normalize(), ToTensorV2()])\n    \n    ld_t = DataLoader(WideTiledDs(t_df, tf, tile_tf, sm), batch_size=CONFIG['batch_size'], shuffle=True, num_workers=4, drop_last=True)\n    ld_v = DataLoader(WideTiledDs(v_df, tf, tile_tf, sm), batch_size=CONFIG['batch_size'], shuffle=False, num_workers=4)\n    \n    m = WideTiledSwin(num_species=ns).cuda()\n    # LOAD PREVIOUS WEIGHTS\n    if CONFIG['resume_path'] != None and os.path.exists(CONFIG['resume_path']):\n        print(f\"Resuming from: {CONFIG['resume_path']}\")\n        sd = torch.load(CONFIG['resume_path'], map_location=CONFIG['device'])\n        m.load_state_dict({k.replace('module.', ''): v for k, v in sd.items()})\n    if torch.cuda.device_count() > 1: m = nn.DataParallel(m)\n    \n    opt = optim.AdamW(m.parameters(), lr=CONFIG['lr'], weight_decay=0.01)\n    scaler = torch.amp.GradScaler('cuda')\n    crits = [nn.HuberLoss(), nn.MSELoss(), nn.BCEWithLogitsLoss()]\n    \n    best_r2 = -float('inf')\n    for e in range(CONFIG['epochs']):\n        m.train(); loss_acc = 0\n        for g, t, b, r, c in ld_t:\n            g, t, b, r, c = g.cuda(), t.cuda(), b.cuda(), r.cuda(), c.cuda()\n            gm, tm, bm, rm, cm, _ = apply_mixup(g, t, b, r, c, ns)\n            opt.zero_grad()\n            with torch.amp.autocast('cuda'):\n                pb, pr, pc = m(gm, tm)\n                loss = crits[0](pb, bm) + 0.1*crits[1](pr, rm) + 0.2*crits[2](pc, cm)\n            scaler.scale(loss).backward(); scaler.unscale_(opt)\n            nn.utils.clip_grad_norm_(m.parameters(), 1.0); scaler.step(opt); scaler.update(); loss_acc += loss.item()\n        \n        m.eval(); ap, at = [], []\n        with torch.no_grad():\n            for g, t, b, _, _ in ld_v:\n                pb, _, _ = m(g.cuda(), t.cuda())\n                ap.append(pb.cpu().numpy()); at.append(b.numpy())\n        \n        y_t, y_p = np.vstack(at), np.vstack(ap)\n        r2 = competition_metric(y_t, y_p)\n        print(f'Fold {fold} | Ep {e+1} | R2: {r2:.4f} | Loss: {loss_acc/len(ld_t):.4f}')\n        if r2 > best_r2: \n            best_r2 = r2\n            sd = m.module.state_dict() if hasattr(m, \"module\") else m.state_dict()\n            torch.save(sd, f\"best_swinv2_widetiled_fold_{fold}.pth\")\n    return best_r2",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "df = pd.read_csv('/kaggle/input/csiro-biomass/train.csv')\ndf_wide = df.pivot_table(index=['image_path','Sampling_Date','Species','Pre_GSHH_NDVI','Height_Ave_cm'], columns='target_name', values='target').reset_index().reindex(columns=['image_path','Sampling_Date','Species','Pre_GSHH_NDVI','Height_Ave_cm'] + TARGET_COLUMNS)\nspecies_map = {s: i for i, s in enumerate(sorted(df_wide['Species'].unique()))}\ngkf = GroupKFold(n_splits=CONFIG['n_splits'])\nfor fold, (t, v) in enumerate(gkf.split(df_wide, groups=df_wide['Sampling_Date'])):\n    print(f\"\\n--- Starting Fold {fold} ---\")\n    score = train_fold(fold, df_wide.iloc[t], df_wide.iloc[v], species_map)\n    print(f\"Fold {fold} Best R2 Score: {score:.4f}\")",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    }
  ]
}