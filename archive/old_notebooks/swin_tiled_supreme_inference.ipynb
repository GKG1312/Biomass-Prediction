{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 112509,
          "databundleVersionId": 14254895,
          "sourceType": "competition"
        },
        {
          "sourceId": 715929,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 544188,
          "modelId": 557286
        },
        {
          "sourceId": 716295,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 544188,
          "modelId": 557286
        }
      ],
      "dockerImageVersionId": 31234,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "1",
      "cell_type": "markdown",
      "source": "# CSIRO Swin-V2 Wide-Tiled Ensemble Inference",
      "metadata": {}
    },
    {
      "id": "2",
      "cell_type": "code",
      "source": "import os\nimport pandas as pd\nimport numpy as np\nimport torch, timm\nimport albumentations as A\nfrom torch.utils.data import Dataset, DataLoader\nfrom albumentations.pytorch import ToTensorV2\nfrom PIL import Image\nimport torch.nn as nn\n\nDATA_DIR = '/kaggle/input/csiro-biomass'\nPATHS = [\n    '/kaggle/input/swinv2-base-tiled/pytorch/default/2/best_swinv2_widetiled_fold_0.pth',\n    '/kaggle/input/swinv2-base-tiled/pytorch/default/2/best_swinv2_widetiled_fold_1.pth',\n    '/kaggle/input/swinv2-base-tiled/pytorch/default/2/best_swinv2_widetiled_fold_2.pth',\n    '/kaggle/input/swinv2-base-tiled/pytorch/default/2/best_swinv2_widetiled_fold_3.pth'\n]\nENSEMBLE_WEIGHTS = [0.15, 0.4, 0.05, 0.4]\nTARGET_COLUMNS = ['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'GDM_g', 'Dry_Total_g']\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-11T06:50:46.717932Z",
          "iopub.execute_input": "2026-01-11T06:50:46.718137Z",
          "iopub.status.idle": "2026-01-11T06:51:36.735554Z",
          "shell.execute_reply.started": "2026-01-11T06:50:46.718119Z",
          "shell.execute_reply": "2026-01-11T06:51:36.734548Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/albumentations/check_version.py:147: UserWarning: Error fetching version info <urlopen error [Errno -3] Temporary failure in name resolution>\n  data = fetch_version_info()\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 1
    },
    {
      "id": "3",
      "cell_type": "code",
      "source": "class WideTiledSwin(nn.Module):\n    def __init__(self, model_name='swinv2_base_window12_192.ms_in22k', num_species=15):\n        super().__init__()\n        # The model is initialized for 384x768 resolution\n        self.backbone = timm.create_model(model_name, pretrained=False, num_classes=0, img_size=(384, 768))\n        d = self.backbone.num_features\n        \n        self.meta_reg = nn.Linear(d*2, 2); self.meta_cls = nn.Linear(d*2, num_species)\n        self.species_emb = nn.Embedding(num_species, 32)\n        \n        fusion_dim = d*2 + 2 + 32\n        self.heads = nn.ModuleList([nn.Sequential(nn.Linear(fusion_dim, 512), nn.GELU(), nn.Linear(512, 1)) for _ in range(5)])\n        \n    def forward(self, x_g, x_wide_tule):\n        # x_g (B, 3, 384, 768) - Global resized view\n        # x_wide_tule (B, 3, 384, 768) - Two high-res tiles concatenated horizontally\n        fg = self.backbone(x_g)\n        ft = self.backbone(x_wide_tule)\n        \n        vis = torch.cat([fg, ft], dim=1)\n        pr, pc = self.meta_reg(vis), self.meta_cls(vis)\n        se = self.species_emb(torch.argmax(pc, dim=1))\n        f = torch.cat([vis, pr, se], dim=1)\n        return torch.cat([h(f) for h in self.heads], dim=1)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4",
      "cell_type": "code",
      "source": "def run_inference():\n    models = []\n    for p in PATHS:\n        if os.path.exists(p):\n            m = WideTiledSwin().to(DEVICE).eval()\n            sd = torch.load(p, map_location=DEVICE)\n            m.load_state_dict({k.replace('module.',''): v for k,v in sd.items()})\n            models.append(m)\n            print(f'Loaded: {p}')\n\n    if not models: return print('No models found!')\n\n    test = pd.read_csv(f'{DATA_DIR}/test.csv')\n    uni = test[['image_path']].drop_duplicates()\n    tf = A.Compose([A.Resize(384, 768), A.Normalize(), ToTensorV2()])\n\n    class InfDs(Dataset):\n        def __len__(self): return len(uni)\n        def __getitem__(self, i):\n            p = uni.iloc[i]['image_path']; img = np.array(Image.open(f'{DATA_DIR}/{p}').convert('RGB'))\n            mid = img.shape[1]//2\n            g = tf(image=img)['image']\n            lt = img[:, :mid]; rt = img[:, mid:]\n            # Tile transform: resize 1:1 to 384x384\n            ttf = A.Compose([A.Resize(384, 384), A.Normalize(), ToTensorV2()])\n            t = torch.cat([ttf(image=lt)['image'], ttf(image=rt)['image']], dim=2)\n            return g, t, p\n\n    ld = DataLoader(InfDs(), batch_size=8)\n    res = []\n    with torch.no_grad():\n        for g, t, ps in ld:\n            fps = []\n            for m in models: fps.append(m(g.to(DEVICE), t.to(DEVICE)).cpu().numpy())\n            w = np.array(ENSEMBLE_WEIGHTS[:len(models)]); w = w / w.sum()\n            avg = np.zeros_like(fps[0])\n            for i in range(len(fps)): avg += fps[i] * w[i]\n\n            for b in range(len(ps)):\n                for i, col in enumerate(TARGET_COLUMNS):\n                    res.append({'image_path': ps[b], 'target_name': col, 'target': max(0.0, float(avg[b,i]))})\n\n    out = pd.DataFrame(res)\n    sub = test[['sample_id', 'image_path', 'target_name']].merge(out, on=['image_path','target_name'], how='left')\n    sub[['sample_id', 'target']].to_csv('submission.csv', index=False)\n    print('Submission saved.')\n\nrun_inference()",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    }
  ]
}