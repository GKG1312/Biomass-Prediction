{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro",
      "metadata": {},
      "source": [
        "# CSIRO SigLIP Biomass Predictor (Log-Target + High-Res Tiling)\n",
        "\n",
        "**Key Strategies:**\n",
        "1. **Backbone**: `vit_siglip_base_patch16_384` (Superior semantic understanding).\n",
        "2. **Target Transform**: `log1p(y)` training to handle heavy-tailed biomass distribution.\n",
        "3. **Tiling Strategy**: `RandomCrop(384)` to feed high-res patches without detail-destroying resizing.\n",
        "4. **Loss**: `WeightedHuberLoss` to match competition metric and be robust to outliers.\n",
        "5. **Hydra Architecture**: Multi-task learning for Species, NDVI/Height, and 5 Biomass targets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "setup",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-14T14:10:48.211429Z",
          "iopub.status.busy": "2026-01-14T14:10:48.210783Z",
          "iopub.status.idle": "2026-01-14T14:10:48.217422Z",
          "shell.execute_reply": "2026-01-14T14:10:48.216791Z",
          "shell.execute_reply.started": "2026-01-14T14:10:48.211405Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os, sys, json, torch, timm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "DATA_DIR = r'D:\\personalProject\\CSIRO-Image2Biomass_Prediction\\csiro-biomass'\n",
        "CHECKPOINT_DIR = r'D:\\personalProject\\CSIRO-Image2Biomass_Prediction\\models_checkpoints'\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "TARGET_COLS = ['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'GDM_g', 'Dry_Total_g']\n",
        "TARGET_WEIGHTS = torch.tensor([0.1, 0.1, 0.1, 0.2, 0.5])\n",
        "\n",
        "CONFIG = {\n",
        "    'model_name': 'vit_base_patch16_siglip_512.v2_webli', \n",
        "    'img_size': 512, \n",
        "    'batch_size': 32, # Large resolution benefits from smaller batches\n",
        "    'lr': 5e-6, \n",
        "    'epochs': 30, \n",
        "    'device': 'cuda'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "model",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-14T14:11:00.781152Z",
          "iopub.status.busy": "2026-01-14T14:11:00.780277Z",
          "iopub.status.idle": "2026-01-14T14:11:00.787307Z",
          "shell.execute_reply": "2026-01-14T14:11:00.786643Z",
          "shell.execute_reply.started": "2026-01-14T14:11:00.781127Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class SigLIPBiomassModel(nn.Module):\n",
        "    def __init__(self, model_name=CONFIG['model_name'], num_species=15, num_region = 4):\n",
        "        super().__init__()\n",
        "        self.backbone = timm.create_model(model_name, pretrained=True, num_classes=0)\n",
        "        d = self.backbone.num_features\n",
        "        \n",
        "        # Auxiliary Tasks\n",
        "        self.meta_reg = nn.Linear(d, 2)     # Predicted NDVI/Height\n",
        "        self.meta_cls = nn.Linear(d, num_species) # Predicted Species\n",
        "        self.species_emb = nn.Embedding(num_species, 32)\n",
        "        self.meta_plc = nn.Linear(d, num_region) # Predicted Species\n",
        "        self.region_emb = nn.Embedding(num_region, 8)\n",
        "        \n",
        "        # 5 Hydra Heads for Biomass (Log-Scale Prediction)\n",
        "        # Fusion: Vis Feat + Meta Reg + Species Emb\n",
        "        fusion_dim = d + 2 + 32 + 8\n",
        "        self.heads = nn.ModuleList([\n",
        "            nn.Sequential(nn.Linear(fusion_dim, 256), nn.GELU(), nn.Linear(256, 1))\n",
        "            for _ in range(5)\n",
        "        ])\n",
        "        \n",
        "    def forward(self, x):\n",
        "        feat = self.backbone(x)\n",
        "        pr = self.meta_reg(feat)\n",
        "        pc = self.meta_cls(feat)\n",
        "        se = self.species_emb(torch.argmax(pc, dim=1))\n",
        "        pplc = self.meta_plc(feat)\n",
        "        plce = self.region_emb(torch.argmax(pplc, dim=1))\n",
        "        \n",
        "        fus = torch.cat([feat, pr, se, plce], dim=1)\n",
        "        out = torch.cat([h(fus) for h in self.heads], dim=1)\n",
        "        \n",
        "        return out, pr, pc, plce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "dataset",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-14T14:11:03.090366Z",
          "iopub.status.busy": "2026-01-14T14:11:03.090105Z",
          "iopub.status.idle": "2026-01-14T14:11:03.096715Z",
          "shell.execute_reply": "2026-01-14T14:11:03.095956Z",
          "shell.execute_reply.started": "2026-01-14T14:11:03.090347Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class BiomassDataset(Dataset):\n",
        "    def __init__(self, df, species_map, region_map, transform=None, is_train=True):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "        self.is_train = is_train\n",
        "        self.species_map = species_map\n",
        "        self.region_map = region_map\n",
        "        \n",
        "    def __len__(self): return len(self.df)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img = np.array(Image.open(os.path.join(DATA_DIR, row['image_path'])).convert('RGB'))\n",
        "        print(type(img))\n",
        "        \n",
        "        if self.transform: \n",
        "            img = self.transform(image=img)['image']\n",
        "        \n",
        "        # Targeted log1p transform\n",
        "        bio = torch.tensor(np.log1p(row[TARGET_COLS].values.astype(np.float32)))\n",
        "        reg = torch.tensor([row['Pre_GSHH_NDVI'], row['Height_Ave_cm']], dtype=torch.float32)\n",
        "        cls = torch.tensor(self.species_map[row['Species']], dtype=torch.long)\n",
        "        plc = torch.tensor(self.region_map[row['State']], dtype=torch.long)\n",
        "        \n",
        "        return img, bio, reg, cls, plc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "training",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-14T14:11:05.189140Z",
          "iopub.status.busy": "2026-01-14T14:11:05.188881Z",
          "iopub.status.idle": "2026-01-14T14:11:05.201091Z",
          "shell.execute_reply": "2026-01-14T14:11:05.200349Z",
          "shell.execute_reply.started": "2026-01-14T14:11:05.189120Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def weighted_huber_loss(pred, target, weights):\n",
        "    huber = nn.HuberLoss(reduction='none')\n",
        "    loss = huber(pred, target)\n",
        "    return (loss * weights.to(pred.device)).mean()\n",
        "\n",
        "def train_fold(fold, t_df, v_df, species_map, region_map):\n",
        "    # Transforms: No Resizing, just Random Crop to 384 (High-Res Details)\n",
        "    t_trans = A.Compose([\n",
        "        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.ColorJitter(brightness=0.1, contrast=0.1, p=0.3),\n",
        "        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "    # Validation: Center Crop to be consistent\n",
        "    v_trans = A.Compose([\n",
        "        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n",
        "        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "    \n",
        "    t_ds = BiomassDataset(t_df, species_map, region_map, transform=t_trans)\n",
        "    v_ds = BiomassDataset(v_df, species_map, region_map, transform=v_trans)\n",
        "    \n",
        "    ld_t = DataLoader(t_ds, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=4, pin_memory=True)\n",
        "    ld_v = DataLoader(v_ds, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=4)\n",
        "    \n",
        "    model = SigLIPBiomassModel(num_species=len(species_map), num_region = len(region_map)).to(CONFIG['device'])\n",
        "\n",
        "    # LOAD PREVIOUS WEIGHTS\n",
        "    # CONFIG['resume_path'] = f\"/kaggle/input/siglip-512/pytorch/default/1/models_checkpoints/siglip_best_fold{fold}.pth\"\n",
        "    # if CONFIG['resume_path'] != None and os.path.exists(CONFIG['resume_path']):\n",
        "    #     print(f\"Resuming from: {CONFIG['resume_path']}\")\n",
        "    #     sd = torch.load(CONFIG['resume_path'], map_location=CONFIG['device'])\n",
        "    #     model.load_state_dict({k.replace('module.', ''): v for k, v in sd.items()})\n",
        "    if torch.cuda.device_count() > 1: model = nn.DataParallel(model)\n",
        "    # Use AdamW with smaller LR for the backbone\n",
        "    opt = optim.AdamW(model.parameters(), lr=CONFIG['lr'], weight_decay=0.01)\n",
        "    scal = torch.amp.GradScaler('cuda')\n",
        "    \n",
        "    best_r2 = -float('inf')\n",
        "\n",
        "    if fold ==2: CONFIG['epochs']=100\n",
        "    \n",
        "    for epoch in range(CONFIG['epochs']):\n",
        "        model.train(); l_acc = 0\n",
        "        for imgs, bios, regs, clss, plcs in ld_t:\n",
        "            imgs, bios, regs, clss, plcs = imgs.to(CONFIG['device']), bios.to(CONFIG['device']), regs.to(CONFIG['device']), clss.to(CONFIG['device']), plcs.to(CONFIG['device'])\n",
        "            opt.zero_grad()\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                p_bio, p_reg, p_cls, p_plc = model(imgs)\n",
        "                l_bio = weighted_huber_loss(p_bio, bios, TARGET_WEIGHTS)\n",
        "                l_reg = nn.MSELoss()(p_reg, regs)\n",
        "                l_cls = nn.CrossEntropyLoss()(p_cls, clss)\n",
        "                l_plc = nn.CrossEntropyLoss()(p_plc, plcs)\n",
        "                loss = l_bio + 0.3*l_reg + 0.1*l_cls + 0.1*l_plcs\n",
        "            scal.scale(loss).backward()\n",
        "            scal.step(opt); scal.update(); l_acc += loss.item()\n",
        "            \n",
        "        model.eval(); all_p, all_t = [], []\n",
        "        with torch.no_grad():\n",
        "            for imgs, bios, _, _, _ in ld_v:\n",
        "                pb, _, _, _ = model(imgs.to(CONFIG['device']))\n",
        "                # Convert back from log scale for metric calculation\n",
        "                all_p.append(torch.expm1(pb).cpu().numpy())\n",
        "                all_t.append(torch.expm1(bios).numpy())\n",
        "        \n",
        "        y_p, y_t = np.vstack(all_p), np.vstack(all_t)\n",
        "        # Weighted R2 Metric\n",
        "        w = TARGET_WEIGHTS.numpy()\n",
        "        ss_res = np.sum(w * (y_t - y_p)**2)\n",
        "        ss_tot = np.sum(w * (y_t - y_t.mean(axis=0))**2)\n",
        "        r2 = 1 - (ss_res / ss_tot)\n",
        "        \n",
        "        print(f'Ep {epoch+1} | Loss: {l_acc/len(ld_t):.4f} | Val R2: {r2:.4f}')\n",
        "        if r2 > best_r2:\n",
        "            best_r2 = r2\n",
        "            torch.save(model.state_dict(), f'{CHECKPOINT_DIR}/siglip_best.pth')\n",
        "    return best_r2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "run",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
        "df_w = df.pivot_table(index=['image_path','Sampling_Date','Species','State','Pre_GSHH_NDVI','Height_Ave_cm'], columns='target_name', values='target').reset_index()\n",
        "print(df_w.head())\n",
        "\n",
        "species_map = {s: i for i, s in enumerate(sorted(df['Species'].unique()))}\n",
        "region_map = {s: i for i, s in enumerate(sorted(df['State'].unique()))}\n",
        "\n",
        "# Simple Train-Val Split (grouped by Sampling_Date)\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n",
        "t_idx, v_idx = next(gss.split(df_w, groups=df_w['Sampling_Date']))\n",
        "\n",
        "t_df, v_df = df_w.iloc[t_idx], df_w.iloc[v_idx]\n",
        "\n",
        "print(f\"Training on {len(t_df)} samples, Validating on {len(v_df)} samples\")\n",
        "score = train_fold(0, t_df, v_df, species_map, region_map)\n",
        "print(f\"Best Val R2 Score: {score:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "databundleVersionId": 14254895,
          "sourceId": 112509,
          "sourceType": "competition"
        },
        {
          "isSourceIdPinned": true,
          "modelId": 560429,
          "modelInstanceId": 547632,
          "sourceId": 720023,
          "sourceType": "modelInstanceVersion"
        }
      ],
      "dockerImageVersionId": 31240,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "pips",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
