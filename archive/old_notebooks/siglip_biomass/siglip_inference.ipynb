{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 112509,
     "databundleVersionId": 14254895,
     "isSourceIdPinned": false,
     "sourceType": "competition"
    },
    {
     "sourceId": 720023,
     "sourceType": "modelInstanceVersion",
     "isSourceIdPinned": false,
     "modelInstanceId": 547632,
     "modelId": 560429
    }
   ],
   "dockerImageVersionId": 31236,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "id": "intro_inf",
   "cell_type": "markdown",
   "source": "# CSIRO SigLIP Inference\nAverage predictions across original high-res tiles for submission.",
   "metadata": {}
  },
  {
   "id": "inf",
   "cell_type": "code",
   "source": "import os\nimport pandas as pd\nimport numpy as np\nimport torch, timm\nimport albumentations as A\nfrom torch.utils.data import Dataset, DataLoader\nfrom albumentations.pytorch import ToTensorV2\nfrom PIL import Image\nimport torch.nn as nn\n\nDATA_DIR = '/kaggle/input/csiro-biomass'\nWEIGHT_PATH = '/kaggle/input/siglip-512/pytorch/default/1/models_checkpoints/siglip_best_fold4.pth'\nTARGET_COLS = ['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'GDM_g', 'Dry_Total_g']\n\nclass SigLIPBiomassModel(nn.Module):\n    def __init__(self, model_name='vit_base_patch16_siglip_gap_512.webli', num_species=15):\n        super().__init__()\n        self.backbone = timm.create_model(model_name, pretrained=False, num_classes=0)\n        d = self.backbone.num_features\n        \n        # Auxiliary Tasks\n        self.meta_reg = nn.Linear(d, 2)     # Predicted NDVI/Height\n        self.meta_cls = nn.Linear(d, num_species) # Predicted Species\n        self.species_emb = nn.Embedding(num_species, 32)\n        \n        # 5 Hydra Heads for Biomass (Log-Scale Prediction)\n        # Fusion: Vis Feat + Meta Reg + Species Emb\n        fusion_dim = d + 2 + 32\n        self.heads = nn.ModuleList([\n            nn.Sequential(nn.Linear(fusion_dim, 256), nn.GELU(), nn.Linear(256, 1))\n            for _ in range(5)\n        ])\n        \n    def forward(self, x):\n        feat = self.backbone(x)\n        pr = self.meta_reg(feat)\n        pc = self.meta_cls(feat)\n        se = self.species_emb(torch.argmax(pc, dim=1))\n        \n        fus = torch.cat([feat, pr, se], dim=1)\n        out = torch.cat([h(fus) for h in self.heads], dim=1)\n        \n        return out\n\nclass SimpleDataset(Dataset):\n    def __init__(self, df, img_dir, transform=None):\n        self.df = df\n        self.img_dir = img_dir\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.df)\n        \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_path = os.path.join(self.img_dir, row['image_path'])\n        img = Image.open(img_path).convert(\"RGB\")\n        img = np.array(img)\n        if self.transform:\n            img = self.transform(image=img)[\"image\"]\n        return img\n\ndef run_inference():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    test_df = pd.read_csv(os.path.join(DATA_DIR, \"test.csv\"))\n    unique_test = test_df.drop_duplicates(subset=['image_path']).copy()\n    \n    model = SigLIPBiomassModel().to(device)\n    if os.path.exists(WEIGHT_PATH):\n        print(f\"Loading weights from: {WEIGHT_PATH}\")\n        state_dict = torch.load(WEIGHT_PATH, map_location=device)\n        # Handle DataParallel prefix\n        state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n        model.load_state_dict(state_dict)\n    else:\n        print(\"WARNING: Checkpoint path not found. Running with random weights!\")\n        \n    model.eval()\n    \n    transform = A.Compose([\n        A.Resize(512, 512),\n        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]), #mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n        ToTensorV2(),\n    ])\n    \n    dataset = SimpleDataset(unique_test, DATA_DIR, transform)\n    loader = DataLoader(dataset, batch_size=8, shuffle=False)\n    \n    results = []\n    with torch.no_grad():\n        for i, image in enumerate(loader):\n            image = image.to(device).float()\n            # Change 2: Predict for the whole batch\n            batch_preds = torch.expm1(model(image)).cpu().numpy() \n            \n            # Change 3: Iterate THROUGH the batch\n            for b in range(batch_preds.shape[0]):\n                # Calculate the exact row in unique_test\n                global_idx = i * loader.batch_size + b\n                img_path = unique_test.iloc[global_idx]['image_path']\n                \n                preds = batch_preds[b]\n                for j, col in enumerate(TARGET_COLS):\n                    results.append({\n                        'image_path': img_path,\n                        'target_name': col,\n                        'target': max(0.0, float(preds[j]))\n                    })\n    \n    pred_df = pd.DataFrame(results)\n    submission = test_df[['sample_id', 'image_path', 'target_name']].merge(pred_df, on=['image_path', 'target_name'], how='left')\n    submission = submission[['sample_id', 'target']]\n    submission.to_csv(\"submission.csv\", index=False)\n    print(\"Preview of submission.csv:\")\n    print(submission.head())\n    print(\"Submission saved to submission.csv\")\n\nif __name__ == \"__main__\":\n    run_inference()",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-14T17:41:43.871916Z",
     "iopub.execute_input": "2026-01-14T17:41:43.872509Z",
     "iopub.status.idle": "2026-01-14T17:41:49.285394Z",
     "shell.execute_reply.started": "2026-01-14T17:41:43.872481Z",
     "shell.execute_reply": "2026-01-14T17:41:49.284705Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Loading weights from: /kaggle/input/siglip-512/pytorch/default/1/models_checkpoints/siglip_best_fold4.pth\nPreview of submission.csv:\n                    sample_id    target\n0  ID1001187975__Dry_Clover_g  0.000000\n1    ID1001187975__Dry_Dead_g  3.240776\n2   ID1001187975__Dry_Green_g  3.473956\n3   ID1001187975__Dry_Total_g  4.024693\n4         ID1001187975__GDM_g  3.590855\nSubmission saved to submission.csv\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 4
  }
 ]
}