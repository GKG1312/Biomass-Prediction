{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 112509,
          "databundleVersionId": 14254895,
          "isSourceIdPinned": false,
          "sourceType": "competition"
        },
        {
          "sourceId": 724679,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 547632,
          "modelId": 560429
        }
      ],
      "dockerImageVersionId": 31236,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "intro",
      "cell_type": "markdown",
      "source": "# CSIRO SigLIP Biomass Predictor (Log-Target + High-Res Tiling)\n\n**Key Strategies:**\n1. **Backbone**: `vit_siglip_base_patch16_384` (Superior semantic understanding).\n2. **Target Transform**: `log1p(y)` training to handle heavy-tailed biomass distribution.\n3. **Tiling Strategy**: `RandomCrop(384)` to feed high-res patches without detail-destroying resizing.\n4. **Loss**: `WeightedHuberLoss` to match competition metric and be robust to outliers.\n5. **Hydra Architecture**: Multi-task learning for Species, NDVI/Height, and 5 Biomass targets.",
      "metadata": {}
    },
    {
      "id": "setup",
      "cell_type": "code",
      "source": [
        "import os, sys, json, torch, timm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "DATA_DIR = '/kaggle/input/csiro-biomass'\n",
        "CHECKPOINT_DIR = './models_checkpoints'\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "TARGET_COLS = ['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'GDM_g', 'Dry_Total_g']\n",
        "TARGET_WEIGHTS = torch.tensor([0.1, 0.1, 0.1, 0.2, 0.5])\n",
        "\n",
        "CONFIG = {\n",
        "    'model_name': 'vit_base_patch16_siglip_512.v2_webli', #timm/ViT-B-16-SigLIP-512\n",
        "    'img_h': 512,\n",
        "    'img_w': 512,\n",
        "    'batch_size': 16, # Large resolution benefits from smaller batches\n",
        "    'lr': 2e-5, \n",
        "    'epochs': 100, \n",
        "    'device': 'cuda'\n",
        "}"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-14T14:10:48.210783Z",
          "iopub.execute_input": "2026-01-14T14:10:48.211429Z",
          "iopub.status.idle": "2026-01-14T14:10:48.217422Z",
          "shell.execute_reply.started": "2026-01-14T14:10:48.211405Z",
          "shell.execute_reply": "2026-01-14T14:10:48.216791Z"
        }
      },
      "outputs": [],
      "execution_count": 7
    },
    {
      "id": "model",
      "cell_type": "code",
      "source": [
        "class SigLIPBiomassModel(nn.Module):\n",
        "    def __init__(self, model_name=CONFIG['model_name'], num_species=15, num_region = 4):\n",
        "        super().__init__()\n",
        "        self.backbone = timm.create_model(model_name, pretrained=True, num_classes=0)\n",
        "        d = self.backbone.num_features\n",
        "        \n",
        "        # Auxiliary Tasks\n",
        "        self.meta_reg = nn.Linear(d, 2)     # Predicted NDVI/Height\n",
        "        self.meta_cls = nn.Linear(d, num_species) # Predicted Species\n",
        "        self.species_emb = nn.Embedding(num_species, 32)\n",
        "        self.meta_plc = nn.Linear(d, num_region) # Predicted Species\n",
        "        self.region_emb = nn.Embedding(num_region, 8)\n",
        "        \n",
        "        # 5 Hydra Heads for Biomass (Log-Scale Prediction)\n",
        "        # Fusion: Vis Feat + Meta Reg + Species Emb\n",
        "        fusion_dim = d + 2 + 32 + 8\n",
        "        self.heads = nn.ModuleList([\n",
        "            nn.Sequential(nn.Linear(fusion_dim, 256), nn.GELU(), nn.Linear(256, 1))\n",
        "            for _ in range(5)\n",
        "        ])\n",
        "        \n",
        "    def forward(self, x, s_lbls=None, r_lbls=None):\n",
        "        feat = self.backbone(x)\n",
        "        pr = self.meta_reg(feat)\n",
        "        pc = self.meta_cls(feat)\n",
        "        se = self.species_emb(s_lbls if s_lbls is not None else torch.argmax(pc, dim=1))\n",
        "        pplc = self.meta_plc(feat)\n",
        "        plce = self.region_emb(r_lbls if r_lbls is not None else torch.argmax(pplc, dim=1))\n",
        "        \n",
        "        fus = torch.cat([feat, pr, se, plce], dim=1)\n",
        "        out = torch.cat([h(fus) for h in self.heads], dim=1)\n",
        "        \n",
        "        return out, pr, pc, plce"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-14T14:11:00.780277Z",
          "iopub.execute_input": "2026-01-14T14:11:00.781152Z",
          "iopub.status.idle": "2026-01-14T14:11:00.787307Z",
          "shell.execute_reply.started": "2026-01-14T14:11:00.781127Z",
          "shell.execute_reply": "2026-01-14T14:11:00.786643Z"
        }
      },
      "outputs": [],
      "execution_count": 9
    },
    {
      "id": "dataset",
      "cell_type": "code",
      "source": [
        "class BiomassDataset(Dataset):\n",
        "    def __init__(self, df, species_map, region_map, transform=None, is_train=True):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "        self.is_train = is_train\n",
        "        self.species_map = species_map\n",
        "        self.region_map = region_map\n",
        "        \n",
        "    def __len__(self): return len(self.df)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img = np.array(Image.open(os.path.join(DATA_DIR, row['image_path'])).convert('RGB'))\n",
        "        # print(type(img))\n",
        "        \n",
        "        if self.transform: \n",
        "            img = self.transform(image=img)['image']\n",
        "        \n",
        "        # Targeted log1p transform\n",
        "        bio = torch.tensor(np.log1p(row[TARGET_COLS].values.astype(np.float32)))\n",
        "        reg = torch.tensor([row['Pre_GSHH_NDVI'], row['Height_Ave_cm']], dtype=torch.float32)\n",
        "        cls = torch.tensor(self.species_map[row['Species']], dtype=torch.long)\n",
        "        plc = torch.tensor(self.region_map[row['State']], dtype=torch.long)\n",
        "        \n",
        "        return img, bio, reg, cls, plc"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-14T14:11:03.090105Z",
          "iopub.execute_input": "2026-01-14T14:11:03.090366Z",
          "iopub.status.idle": "2026-01-14T14:11:03.096715Z",
          "shell.execute_reply.started": "2026-01-14T14:11:03.090347Z",
          "shell.execute_reply": "2026-01-14T14:11:03.095956Z"
        }
      },
      "outputs": [],
      "execution_count": 10
    },
    {
      "id": "training",
      "cell_type": "code",
      "source": [
        "def weighted_huber_loss(pred, target, weights):\n",
        "    huber = nn.HuberLoss(reduction='none')\n",
        "    loss = huber(pred, target)\n",
        "    return (loss * weights.to(pred.device)).mean()\n",
        "\n",
        "def train_fold(fold, t_df, v_df, species_map, region_map):\n",
        "    # Transforms: No Resizing, just Random Crop to 384 (High-Res Details)\n",
        "    t_trans = A.Compose([\n",
        "        A.Resize(CONFIG['img_h'], CONFIG['img_w']),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.ColorJitter(brightness=0.1, contrast=0.1, p=0.3),\n",
        "        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "    # Validation: Center Crop to be consistent\n",
        "    v_trans = A.Compose([\n",
        "        A.Resize(CONFIG['img_h'], CONFIG['img_w']),\n",
        "        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "    \n",
        "    t_ds = BiomassDataset(t_df, species_map, region_map, t_trans)\n",
        "    v_ds = BiomassDataset(v_df, species_map, region_map, v_trans)\n",
        "    \n",
        "    ld_t = DataLoader(t_ds, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=4, pin_memory=True)\n",
        "    ld_v = DataLoader(v_ds, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=4)\n",
        "    \n",
        "    model = SigLIPBiomassModel(num_species=len(species_map), num_region = len(region_map)).to(CONFIG['device'])\n",
        "\n",
        "    # LOAD PREVIOUS WEIGHTS\n",
        "    # CONFIG['resume_path'] = f\"/kaggle/input/siglip-512/pytorch/default/6/models_checkpoints/siglip_best_fold0.pth\"\n",
        "    # if CONFIG['resume_path'] != None and os.path.exists(CONFIG['resume_path']):\n",
        "    #     print(f\"Resuming from: {CONFIG['resume_path']}\")\n",
        "    #     sd = torch.load(CONFIG['resume_path'], map_location=CONFIG['device'])\n",
        "    #     model.load_state_dict({k.replace('module.', ''): v for k, v in sd.items()})\n",
        "    if torch.cuda.device_count() > 1: model = nn.DataParallel(model)\n",
        "    # Use AdamW with smaller LR for the backbone\n",
        "    opt = optim.AdamW(model.parameters(), lr=CONFIG['lr'], weight_decay=0.01)\n",
        "    # ADD THIS: OneCycleLR Scheduler (Steps per batch)\n",
        "    # Smoother Scheduler for Small Data\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=CONFIG['epochs'], eta_min=1e-7)\n",
        "    scal = torch.amp.GradScaler('cuda')\n",
        "    \n",
        "    best_r2 = -float('inf')\n",
        "\n",
        "    # if fold ==2: CONFIG['epochs']=100\n",
        "    \n",
        "    for epoch in range(CONFIG['epochs']):\n",
        "        model.train(); l_acc = 0\n",
        "        for imgs, bios, regs, clss, plcs in ld_t:\n",
        "            imgs, bios, regs, clss, plcs = imgs.to(CONFIG['device']), bios.to(CONFIG['device']), regs.to(CONFIG['device']), clss.to(CONFIG['device']), plcs.to(CONFIG['device'])\n",
        "            opt.zero_grad()\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                p_bio, p_reg, p_cls, p_plc = model(imgs, s_lbls=clss, r_lbls=plcs)\n",
        "                l_bio = weighted_huber_loss(p_bio, bios, TARGET_WEIGHTS)\n",
        "                l_reg = nn.MSELoss()(p_reg, regs)\n",
        "                l_cls = nn.CrossEntropyLoss()(p_cls, clss)\n",
        "                l_plc = nn.CrossEntropyLoss()(p_plc, plcs)\n",
        "                loss = l_bio + 0.3*l_reg + 0.1*l_cls + 0.1*l_plc\n",
        "            scal.scale(loss).backward()\n",
        "            scal.step(opt)\n",
        "            scal.update()\n",
        "            l_acc += loss.item()\n",
        "            \n",
        "        model.eval(); all_p, all_t = [], []\n",
        "        with torch.no_grad():\n",
        "            for imgs, bios, _, _, _ in ld_v:\n",
        "                pb, _, _, _ = model(imgs.to(CONFIG['device']))\n",
        "                # Convert back from log scale for metric calculation\n",
        "                # Clip log-predictions between 0 and 10 to prevent infinite linear values\n",
        "                pb = torch.clamp(pb, 0, 10)\n",
        "                all_p.append(torch.expm1(pb).cpu().numpy())\n",
        "                all_t.append(torch.expm1(bios).numpy())\n",
        "        \n",
        "        y_p, y_t = np.vstack(all_p), np.vstack(all_t)\n",
        "        # Weighted R2 Metric\n",
        "        w = TARGET_WEIGHTS.numpy()\n",
        "        ss_res = np.sum(w * (y_t - y_p)**2)\n",
        "        ss_tot = np.sum(w * (y_t - y_t.mean(axis=0))**2)\n",
        "        r2 = 1 - (ss_res / ss_tot)\n",
        "        \n",
        "        scheduler.step() # Step per epoch for Cosine\n",
        "        print(f'Fold {fold} | Ep {epoch+1} | Loss: {l_acc/len(ld_t):.4f} | Val R2: {r2:.4f}')\n",
        "        if r2 > best_r2:\n",
        "            best_r2 = r2\n",
        "            torch.save(model.state_dict(), f'{CHECKPOINT_DIR}/siglip_best_fold{fold}.pth')\n",
        "    return best_r2"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-14T14:11:05.188881Z",
          "iopub.execute_input": "2026-01-14T14:11:05.189140Z",
          "iopub.status.idle": "2026-01-14T14:11:05.201091Z",
          "shell.execute_reply.started": "2026-01-14T14:11:05.189120Z",
          "shell.execute_reply": "2026-01-14T14:11:05.200349Z"
        }
      },
      "outputs": [],
      "execution_count": 11
    },
    {
      "id": "run",
      "cell_type": "code",
      "source": [
        "# df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
        "# df_w = df.pivot_table(index=['image_path','Sampling_Date','Species','State','Pre_GSHH_NDVI','Height_Ave_cm'], columns='target_name', values='target').reset_index()\n",
        "# print(df_w.head())\n",
        "# species_map = {s: i for i, s in enumerate(sorted(df['Species'].unique()))}\n",
        "# region_map = {s: i for i, s in enumerate(sorted(df['State'].unique()))}\n",
        "\n",
        "\n",
        "# gkf = GroupKFold(n_splits=5)\n",
        "# for fold, (t_idx, v_idx) in enumerate(gkf.split(df_w, groups=df_w['Sampling_Date'])):\n",
        "#     # if fold == 2:\n",
        "#     print(f\"----Training fold {fold}----\")\n",
        "#     score = train_fold(fold, df_w.iloc[t_idx], df_w.iloc[v_idx], species_map, region_map)\n",
        "#     print(f\"Fold {fold} Best R2 Score: {score:.4f}\")\n",
        "#     # else:\n",
        "#     #     continue\n",
        "    \n",
        "    "
      ],
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c7d63a61-04cc-4394-a140-ed1346287f5b",
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
        "df_w = df.pivot_table(index=['image_path','Sampling_Date','Species','State','Pre_GSHH_NDVI','Height_Ave_cm'], columns='target_name', values='target').reset_index()\n",
        "print(df_w.head())\n",
        "\n",
        "species_map = {s: i for i, s in enumerate(sorted(df['Species'].unique()))}\n",
        "region_map = {s: i for i, s in enumerate(sorted(df['State'].unique()))}\n",
        "\n",
        "# Simple Train-Val Split (grouped by Sampling_Date)\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n",
        "t_idx, v_idx = next(gss.split(df_w, groups=df_w['Sampling_Date']))\n",
        "\n",
        "t_df, v_df = df_w.iloc[t_idx], df_w.iloc[v_idx]\n",
        "\n",
        "print(f\"Training on {len(t_df)} samples, Validating on {len(v_df)} samples\")\n",
        "score = train_fold(0, t_df, v_df, species_map, region_map)\n",
        "print(f\"Best Val R2 Score: {score:.4f}\")"
      ],
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}